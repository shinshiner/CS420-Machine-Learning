\documentclass[12pt,a4paper]{article}
\usepackage{amsmath,amscd,amsbsy,amssymb,latexsym,url,bm,amsthm}
\usepackage{epsfig,graphicx,subfigure}
\usepackage{enumitem,balance}
\usepackage{wrapfig}
\usepackage{mathrsfs, euscript}
\usepackage[usenames]{xcolor}
\usepackage{hyperref}
\usepackage[vlined,ruled,commentsnumbered,linesnumbered]{algorithm2e}
\usepackage{float}
\usepackage{array}
\usepackage{diagbox}
\usepackage{color}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{gensymb}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{aurical}
\usepackage{times}
\usepackage{caption}
\usepackage{fontspec}
\usepackage{booktabs}
\setmainfont{Times New Roman}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{exercise}{Exercise}[section]
\newtheorem*{solution}{Solution}
\theoremstyle{definition}

\newcommand{\postscript}[2]
 {\setlength{\epsfxsize}{#2\hsize}
  \centerline{\epsfbox{#1}}}
\renewcommand{\baselinestretch}{1.0}

\setlength{\oddsidemargin}{-0.365in}
\setlength{\evensidemargin}{-0.365in}
\setlength{\topmargin}{-0.3in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{10.1in}
\setlength{\textwidth}{7in}
\makeatletter \renewenvironment{proof}[1][Proof] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother
\makeatletter
\renewenvironment{solution}[1][Solution] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother

\begin{document}
\noindent
%==========================================================
\noindent\framebox[\linewidth]{\shortstack[c]{
\Large{\textbf{Report on Homework 2}}\vspace{1mm}\\ 
CS420, Machine Learning, Shikui Tu, Summer 2018 \vspace{1mm} \\
Zelin Ye 515030910468}}

\section{PCA algorithm}

When it comes to principal component, the most common used approach is principal component analysis (PCA) algorithm. Thus, I can use the original PCA algorithm to extract the first principal component of the dataset. The computational details can refer to Alg. \ref{alg:pca_1}.

\vspace{0.01\linewidth}
\begin{algorithm}[H]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\caption{Original PCA}
	\label{alg:pca_1}
	\vspace{0.25\baselineskip}
	
	\Input{The dataset $X$, a $n \times N$ matrix}
	\Output{The first principal component $w$}
	%Rearrange the dataset $X$ by
	%\begin{equation*}
	%	\centering
	%	X \leftarrow X^T,
	%\end{equation*}
	%so that each line denotes one data point;
	
	Conduct normalization for $X$, and make sure the mean of $X$ is 0;
	
	Find the covariance matrix of $X$, denoted by $C$:
	\begin{equation*}
		\centering
		C = XX^{T};
	\end{equation*}
	
	Calculate the eigenvalues $\lambda$ and eigenvectors $V$ of $X$;
	
	Choose the maximal eigenvalue $\lambda_{m}$ and corresponding eigenvector $v_{m}$;
	
	Calculate the first principal component:
	\begin{equation*}
		\centering
		w = v_{m}^{T}X;
	\end{equation*}
	
	\Return $w$;
\end{algorithm}
\vspace{0.01\linewidth}

The above algorithm (PCA) is a classical and commonly used method to solve principal components, which has the following characteristics.

\vspace{-0.012\linewidth}
\paragraph{Pros:}
\begin{enumerate}
	\item PCA has simple logic and is easy to implement;
	
	\item The orthogonality among principal components chosen by PCA can eliminate the interaction between original data components;
	
	\item PCA belongs to unsupervised learning, and it is not restricted by sample labels.
\end{enumerate}

\vspace{-0.03\linewidth}
\paragraph{Cons:}
\begin{enumerate}
	\item PCA treats all samples, namely the collection of eigenvectors, as a whole and neglects the category attributes. Nevertheless, the projection direction it neglects might contain some important separability information;
	
	\item PCA would be time-consuming when encountering large amount of data;
	
	\item The actual meanings of principal components extracted by PCA are a little bit ad-hoc and hard to explain.
\end{enumerate}

The original PCA might have good performance when handling linear data, but it would encounter difficulties under non-linear data. In non-linear cases, a variant of PCA called KPCA (kernel principal components analysis) shows its strength.

The innovation of KPCA is that it introduces a non-linear mapping function $\phi(x)$, mapping the data from original space to high-dimensional space. Besides, the deduction of KPCA takes advantage of the theorem that \textit{each vector in the space could be expressed linearly by all the samples in the space}. The details of KPCA can refer to Alg. \ref{alg:pca_2}.

\vspace{0.01\linewidth}
\begin{algorithm}[H]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\caption{KPCA}
	\label{alg:pca_2}
	\vspace{0.25\baselineskip}
	
	\Input{The dataset $X$, a $n \times N$ matrix; kernel function $\phi(x)$}
	\Output{The first principal component $w$}	
	Conduct normalization for $X$, and make sure the mean of $X$ is 0;
	
	Calculate kernel matrix $K$:
	\begin{equation*}
		\centering
		K = X^{T}X,
	\end{equation*}
	where X=[$\phi(x_1),...,\phi(x_N)$];
	
	Find the covariance matrix of $X$, denoted by $C$:
	\begin{equation*}
		\centering
		C = XX^{T};
	\end{equation*}
	
	Calculate the eigenvalues $\lambda$ and eigenvectors $U$ of $K$;
	
	Choose the maximal eigenvalue $\lambda_{m}$ and calculate corresponding eigenvector $u_{m}$;
	
	Calculate the corresponding eigenvectors $v_m$ of covariance matrix $C$ by $u_m$:
	\begin{align*}
		\centering
		v_m &= \dfrac{1}{\|X^Tu_m\|}X^Tu_m \\
		&= \dfrac{1}{\sqrt{u_m^TXX^Tu_m}}X^Tu_m \\
		&= \dfrac{1}{\sqrt{u_m^T(\lambda_m u_m)}}X^Tu_m \\
		&= \dfrac{1}{\sqrt{\lambda_m}}X^Tu_m;
	\end{align*}
		
	Calculate the first principal component:
	\begin{equation*}
		\centering
		w = v_{m}^{T}X;
	\end{equation*}
	
	\Return $w$;
\end{algorithm}
\vspace{0.01\linewidth}

KPCA is a ingenious extension of PCA, and is also widely used (e.g. dimension reduction, clustering). The pros and cons of KPCA are as follows.

\vspace{-0.012\linewidth}
\paragraph{Pros:}
\begin{enumerate}
	\item Basically, KPCA owns almost all of the advantages of PCA;

	\item KPCA has a stronger universality, which could find out the non-linear information contained in dataset;
	
	\item KPCA uses simple kernel functions (e.g. polynomial kernel function) to realize complex non-linear transform;
\end{enumerate}

\vspace{-0.03\linewidth}
\paragraph{Cons:}
\begin{enumerate}
	\item The logic and implementation of KPCA is a little bit complicated;
	
	\item The dimension of kernel matrix $K$ is $N \times N$ ($N$ is the number of samples). It might take quantities of time to process $K$ when handling large scale of samples;
	
	\item Different choices of kernel functions and parameters would affect the result to a certain extent, which makes this algorithm more tricky.
	
	\item Same as PCA, the actual meanings of principal components extracted by KPCA are also inexplicable.
\end{enumerate}

\section{Factor Analysis (FA)}

\vspace{-0.03\linewidth}
\begin{large}
\begin{align*}
	\centering	
	p(y|x) &= \dfrac{p(x|y)p(y)}{p(x)} \\
	&= \dfrac{G(x|Ay+\mu,\Sigma_{e})G(y|0,\Sigma_{y})}{p(x)} \\
	&= \dfrac{G(x|Ay+\mu,\Sigma_{e})G(y|0,\Sigma_{y})}{G(x|\mu+\mu_{e},AA^{T}\Sigma_{y}+\Sigma_{e})}
\end{align*}
\end{large}

where $\mu_{e}$ denotes the mean value of $e$, generally considered to be 0.

\section{Independent Component Analysis (ICA)}

ICA aims to decompose the source signal into independent parts. If the source signals are non-Gaussian, the decomposition is unique, or there would be a variety of such decompositions.

Suppose the source signal $s$ consists of two components, conforming to multi-valued normal distribution, namely $s \sim N(0,I)$. Obviously, the probablity density funciton of $s$ is centered on the mean 0, and the projection plane is an ellipse.

Meanwhile, we have $x=As$, where $x$ denotes the actual signals received while $A$ represents a mixing matrix. Then $x$ is also Gaussian, with a mean of 0 and a covariance of $E[xx^{T}]=E[Ass^{T}A^{T}]=AA^{T}$.

Let $C$ be a orthogonal matrix, and $A^{'}=AR$. If $A$ is replaced by $A{'}$, then we can get $x^{'}=A^{'}s$. It is easy to find that $x^{'}$ also conforms to normal distribution, with a mean of 0 and a covariance of $E[x^{'}(x^{'})^{T}]=E[A^{'}ss^{T}(A^{'})^{T}]=E[ACss^{T}(AC)^{T}]=ACC^{T}A^{T}=AA^{T}$.

Apparently, $x$ and $x^{'}$ conform to the same distribution with different mixing matrices. Then we cannot determine the mixing matrix or the source signals from the received signals. Nevertheless, if $x$ is non-Gaussian (e.g. Uniform Distribution), such case would be effectively avoided. Therefore, maximizing non-Gaussianity should be used as a principle for ICA estimation.

\section{Causal Discovery Algorithms}

\subsection{Problem Description}

Generally speaking, the housing price are affected by many factors, such as the number of rooms, crime rate, pupil-teacher ratio and etc. There also exist some causalities among those factors. It is beneficial for further mastering the housing market if one can figure out these causalities. However, such causalities are often difficult to determine intuitively, it is thus necessary to conduct causal discovery on these factors.

I utilize the factors and corresponding values in the Boston Housing dataset, which is available at the UCI Repository \cite{boston}. The details of the dataset can refer to Appendix \ref{apd:boston}.

\subsection{Approach}

\subsection{Experiments}

\section{Causal Tree Reconstruction}

pass

\newpage
\begin{appendix}
\section{Details of the Boston Housing Dataset}
\label{apd:boston}

The Boston Housing Dataset consists of 506 samples, each has 12 features, namely the factors that affect housing price. Table \ref{tab:boston} shows a detailed description of these features.

\begin{table}[H]
	\renewcommand\arraystretch{1.35}
	\caption{Details of the Features in Boston Housing Dataset}
	\label{tab:boston}
	\centering
	
	\begin{tabular}{c|c|c}
		\centering
		Identifier & Feature Names & Description \\
		\hline
		1 & CRIM & per capita crime rate by town \\
		2 & ZN & proportion of residential land zoned for lots over 25,000 sq.ft. \\
		3 & INDUS & proportion of non-retail business acres per town \\
		4 & NOX & nitric oxides concentration (parts per 10 million) \\
		5 & RM & average number of rooms per dwelling \\
		6 & AGE & proportion of owner-occupied units built prior to 1940 \\
		7 & DIS & weighted distances to five Boston employment centres \\
		8 & TAX & full-value property-tax rate per \$10,000 \\
		9 & PTRATIO & pupil-teacher ratio by town \\
		10 & B & 1000(Bk - 0.63)\^2 where Bk is the proportion of blacks by town \\
		11 & LSTAT & lower status of the population \\			
		12 & MEDV & Median value of owner-occupied homes in \$1000's \\
	\end{tabular}
\end{table}

\end{appendix}

\bibliographystyle{ieeetr}
\bibliography{bio}

\end{document}