\documentclass[12pt,a4paper]{article}
\usepackage{amsmath,amscd,amsbsy,amssymb,latexsym,url,bm,amsthm}
\usepackage{epsfig,graphicx,subfigure}
\usepackage{enumitem,balance}
\usepackage{wrapfig}
\usepackage{mathrsfs, euscript}
\usepackage[usenames]{xcolor}
\usepackage{hyperref}
\usepackage[vlined,ruled,commentsnumbered,linesnumbered]{algorithm2e}
\usepackage{float}
\usepackage{array}
\usepackage{diagbox}
\usepackage{color}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{gensymb}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{aurical}
\usepackage{times}
\usepackage{caption}
\usepackage{fontspec}
\usepackage{booktabs}
\setmainfont{Times New Roman}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{exercise}{Exercise}[section]
\newtheorem*{solution}{Solution}
\theoremstyle{definition}

\newcommand{\postscript}[2]
 {\setlength{\epsfxsize}{#2\hsize}
  \centerline{\epsfbox{#1}}}
\renewcommand{\baselinestretch}{1.0}

\setlength{\oddsidemargin}{-0.365in}
\setlength{\evensidemargin}{-0.365in}
\setlength{\topmargin}{-0.3in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{10.1in}
\setlength{\textwidth}{7in}
\makeatletter \renewenvironment{proof}[1][Proof] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother
\makeatletter
\renewenvironment{solution}[1][Solution] {\par\pushQED{\qed}\normalfont\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{.}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother

\begin{document}
\noindent
%==========================================================
\noindent\framebox[\linewidth]{\shortstack[c]{
\Large{\textbf{Report on Homework 2}}\vspace{1mm}\\ 
CS420, Machine Learning, Shikui Tu, Summer 2018 \vspace{1mm} \\
Zelin Ye 515030910468}}

\section{PCA algorithm}

When it comes to principal component, the most common used approach is principal component analysis (PCA) algorithm. Thus, I can use the original PCA algorithm to extract the first principal component of the dataset. The computational details can refer to Alg. \ref{alg:pca_1}.

\vspace{0.01\linewidth}
\begin{algorithm}[H]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\caption{Original PCA}
	\label{alg:pca_1}
	\vspace{0.25\baselineskip}
	
	\Input{The dataset $X$, a $n \times N$ matrix}
	\Output{The first principal component $w$}
	%Rearrange the dataset $X$ by
	%\begin{equation*}
	%	\centering
	%	X \leftarrow X^T,
	%\end{equation*}
	%so that each line denotes one data point;
	
	Conduct normalization for $X$, and make sure the mean of $X$ is 0;
	
	Find the covariance matrix of $X$, denoted by $C$:
	\begin{equation*}
		\centering
		C = XX^{T};
	\end{equation*}
	
	Calculate the eigenvalues $\lambda$ and eigenvectors $V$ of $X$;
	
	Choose the maximal eigenvalue $\lambda_{m}$ and corresponding eigenvector $v_{m}$;
	
	Calculate the first principal component:
	\begin{equation*}
		\centering
		w = v_{m}^{T}X;
	\end{equation*}
	
	\Return $w$;
\end{algorithm}
\vspace{0.01\linewidth}

The above algorithm (PCA) is a classical and commonly used method to solve principal components, which has the following characteristics.

\vspace{-0.012\linewidth}
\paragraph{Pros:}
\begin{enumerate}
	\item PCA has simple logic and is easy to implement;
	
	\item The orthogonality among principal components chosen by PCA can eliminate the interaction between original data components;
	
	\item PCA belongs to unsupervised learning, and it is not restricted by sample labels.
\end{enumerate}

\vspace{-0.03\linewidth}
\paragraph{Cons:}
\begin{enumerate}
	\item PCA treats all samples, namely the collection of eigenvectors, as a whole and neglects the category attributes. Nevertheless, the projection direction it neglects might contain some important separability information;
	
	\item PCA would be time-consuming when encountering large amount of data;
	
	\item The actual meanings of principal components extracted by PCA are a little bit ad-hoc and hard to explain.
\end{enumerate}

The original PCA might have good performance when handling linear data, but it would encounter difficulties under non-linear data. In non-linear cases, a variant of PCA called KPCA (kernel principal components analysis) shows its strength.

The innovation of KPCA is that it introduces a non-linear mapping function $\phi(x)$, mapping the data from original space to high-dimensional space. Besides, the deduction of KPCA takes advantage of the theorem that \textit{each vector in the space could be expressed linearly by all the samples in the space}. The details of KPCA can refer to Alg. \ref{alg:pca_2}.

\vspace{0.01\linewidth}
\begin{algorithm}[H]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\caption{KPCA}
	\label{alg:pca_2}
	\vspace{0.25\baselineskip}
	
	\Input{The dataset $X$, a $n \times N$ matrix; kernel function $\phi(x)$}
	\Output{The first principal component $w$}	
	Conduct normalization for $X$, and make sure the mean of $X$ is 0;
	
	Calculate kernel matrix $K$:
	\begin{equation*}
		\centering
		K = X^{T}X,
	\end{equation*}
	where X=[$\phi(x_1),...,\phi(x_N)$];
	
	Find the covariance matrix of $X$, denoted by $C$:
	\begin{equation*}
		\centering
		C = XX^{T};
	\end{equation*}
	
	Calculate the eigenvalues $\lambda$ and eigenvectors $U$ of $K$;
	
	Choose the maximal eigenvalue $\lambda_{m}$ and calculate corresponding eigenvector $u_{m}$;
	
	Calculate the corresponding eigenvectors $v_m$ of covariance matrix $C$ by $u_m$:
	\begin{align*}
		\centering
		v_m &= \dfrac{1}{\|X^Tu_m\|}X^Tu_m \\
		&= \dfrac{1}{\sqrt{u_m^TXX^Tu_m}}X^Tu_m \\
		&= \dfrac{1}{\sqrt{u_m^T(\lambda_m u_m)}}X^Tu_m \\
		&= \dfrac{1}{\sqrt{\lambda_m}}X^Tu_m;
	\end{align*}
		
	Calculate the first principal component:
	\begin{equation*}
		\centering
		w = v_{m}^{T}X;
	\end{equation*}
	
	\Return $w$;
\end{algorithm}
\vspace{0.01\linewidth}

KPCA is a ingenious extension of PCA, and is also widely used (e.g. dimension reduction, clustering). The pros and cons of KPCA are as follows.

\vspace{-0.012\linewidth}
\paragraph{Pros:}
\begin{enumerate}
	\item Basically, KPCA owns almost all of the advantages of PCA;

	\item KPCA has a stronger universality, which could find out the non-linear information contained in dataset;
	
	\item KPCA uses simple kernel functions (e.g. polynomial kernel function) to realize complex non-linear transform;
\end{enumerate}

\vspace{-0.03\linewidth}
\paragraph{Cons:}
\begin{enumerate}
	\item The logic and implementation of KPCA is a little bit complicated;
	
	\item The dimension of kernel matrix $K$ is $N \times N$ ($N$ is the number of samples). It might take quantities of time to process $K$ when handling large scale of samples;
	
	\item Different choices of kernel functions and parameters would affect the result to a certain extent, which makes this algorithm more tricky.
	
	\item Same as PCA, the actual meanings of principal components extracted by KPCA are also inexplicable.
\end{enumerate}

\section{Factor Analysis (FA)}

\vspace{-0.03\linewidth}
\begin{large}
\begin{align*}
	\centering	
	p(y|x) &= \dfrac{p(x|y)p(y)}{p(x)} \\
	&= \dfrac{G(x|Ay+\mu,\Sigma_{e})G(y|0,\Sigma_{y})}{p(x)} \\
	&= \dfrac{G(x|Ay+\mu,\Sigma_{e})G(y|0,\Sigma_{y})}{G(x|\mu+\mu_{e},AA^{T}\Sigma_{y}+\Sigma_{e})}
\end{align*}
\end{large}

where $\mu_{e}$ denotes the mean value of $e$, generally considered to be 0.

\section{Independent Component Analysis (ICA)}

ICA aims to decompose the source signal into independent parts. If the source signals are non-Gaussian, the decomposition is unique, or there would be a variety of such decompositions.

Suppose the source signal $s$ consists of two components, conforming to multi-valued normal distribution, namely $s \sim N(0,I)$. Obviously, the probablity density funciton of $s$ is centered on the mean 0, and the projection plane is an ellipse.

Meanwhile, we have $x=As$, where $x$ denotes the actual signals received while $A$ represents a mixing matrix. Then $x$ is also Gaussian, with a mean of 0 and a covariance of $E[xx^{T}]=E[Ass^{T}A^{T}]=AA^{T}$.

Let $C$ be a orthogonal matrix, and $A^{'}=AR$. If $A$ is replaced by $A{'}$, then we can get $x^{'}=A^{'}s$. It is easy to find that $x^{'}$ also conforms to normal distribution, with a mean of 0 and a covariance of $E[x^{'}(x^{'})^{T}]=E[A^{'}ss^{T}(A^{'})^{T}]=E[ACss^{T}(AC)^{T}]=ACC^{T}A^{T}=AA^{T}$.

Apparently, $x$ and $x^{'}$ conform to the same distribution with different mixing matrices. Then we cannot determine the mixing matrix or the source signals from the received signals. Nevertheless, if $x$ is non-Gaussian (e.g. Uniform Distribution), such case would be effectively avoided. Therefore, maximizing non-Gaussianity should be used as a principle for ICA estimation.

\section{Causal discovery algorithms}

pass

\section{Causal tree reconstruction}

pass

\end{document}